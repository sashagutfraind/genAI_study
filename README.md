# Generative AI - core study sheet

Generative AI (including language, image, time etc) is massive new area of data science, with its own curriculum. 
Below is a core curriculum for enter this field

Subject areas
- data preparation and hardware
- neural architectures
- training and loss
- prompting techniques
- efficient serving
- evaluation
- software architectures


# Preparation
- toolkits: Torch, Tensorflow, Jax, MXNet
- linear algebra for deep learning: tensor multiplication
- tokenization
- byte-pair encoding
- data augmentation
- GPU computation


# Neural architectures
- transformers
- encoder and decoder
- large language models (LLMs)
- CNN
- RNN
- variational auto-encoders
- masked language modeling
- dropout
- activation functions in hidden layers
- activation functions in output layer
- quantization

# Pre-training
- loss functions
- optimizers
- training schedules
- teacher forcing
- loss spike and stabilization
- vanishing gradients

# Fine-tuning
- parameter-efficient fine-tuning, e.g. LoRA
- supervised fine-tuning
- RLHF
- instruction following
- in-context learning
- zero and few-shot learning
- prompting techniques, e.g. chain-of-thought, medprompt
- LLMP output evaluation, e.g. critics, ROUGE 

# Software solutions
- hallucinations and countermeasures
- toxicity 
- vectorstore and cosine similarity
- retrieval-augmented generation (RAG)
- efficient inference
- multi-agent systems

# Others
## Methods
- distillation
- PPO, DPO


## Special architectures
- CLIP
- ResNet
- twin output models: Siamese, TarNet
- ViT visual transformer
