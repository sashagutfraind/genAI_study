# Generative AI - minimal study sheet

Generative AI (including language, image, time etc) is massive new area of data science, with its own curriculum. Training in classical ML is helpful but is insufficient for understanding the theory and implementation of generative AI methods.

# Topic list
- data preparation and hardware
- neural architectures
- training and loss
- prompting techniques
- efficient serving
- evaluation
- software architectures


# Preparation
- toolkits: Torch, Tensorflow, Jax, MXNet
- linear algebra for deep learning: tensor multiplication
- tokenization
- byte-pair encoding
- data augmentation
- GPU computation


# Neural architectures
- quantization
- transformers
- encode and decoder
- large language models (LLMs)
- CNN
- RNN
- notable models: CLIP, Siamese
- variational auto-encoders
- masked language modeling
- dropout
- activation functions in hidden layers: ReLU, GLU
- activation functions in output layer: logit, softmax, linear

# Pre-training
- loss functions
- optimizers
- training schedules
- teacher forcing
- loss spike and stabilization
- vanishing gradients

# Fine-tuning
- adapters
- parameter-efficient fine-tuning
- instruction following
- in-context learning
- prompting techniques
	- chain-of-thought
	- medprompt
- zero and few-shot learning
- supervised fine-tuning
- RLHF
- output evaluation with NLP 

# Software architectures
- hallucinations and countermeasures
- toxicity 
- vectorstore and cosine similarity
- retrieval-augmented generation (RAG)
- efficient inference
- multi-agent systems

# Others
- distillation

- 
